{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from source.preprocessing import splitter, Converter\n",
    "from source.datamodels import iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocessing\n",
    "load datasets, convert third-party data files to our format etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "project_folder = \"F:/PythonNotebooks/Study/Quantum/Bearings/\"\n",
    "own_data_path = os.path.join(project_folder, \"data/own datasets/\")\n",
    "third_party_data_path = os.path.join(project_folder, \"data/third party datasets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load our initial datasets\n",
    "Datasets obtained from our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signals_dataset = pd.read_csv(os.path.join(own_data_path, 'bearing_signals.csv'))\n",
    "classes_dataset = pd.read_csv(os.path.join(own_data_path, 'bearing_classes.csv'), delimiter=';', skiprows=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Third-party datasets\n",
    "Load converted third-party datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "## Signals and classes datasets join\n",
    "Use to combine our datasets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "targets_map = dict(zip(classes_dataset['bearing_id'], classes_dataset['status']))\n",
    "targets_vector = signals_dataset['bearing_2_id'].map(targets_map)\n",
    "joined_dataset = signals_dataset.copy()\n",
    "joined_dataset.insert(loc=0, column='target', value=targets_vector)\n",
    "joined_dataset.to_csv(os.path.join(own_data_path, 'bearings.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "## Convert third-party data files to our standard dataframe view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cesar_1_path = os.path.join(third_party_data_path, 'Bearings_cesar_1')\n",
    "cesar_1 = Converter.cesar_convert(cesar_1_path)\n",
    "\n",
    "cesar_2_path = os.path.join(third_party_data_path, 'Bearings_cesar_1')\n",
    "cesar_2 = Converter.cesar_convert(cesar_2_path)\n",
    "\n",
    "luigi_path = os.path.join(third_party_data_path, 'Bearings_luigi')\n",
    "luigi = Converter.luigi_convert(luigi_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "## Split datasets\n",
    "Split datasets on chunks and evaluate set of statistical features for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%% time\n",
    "\n",
    "# stats = ['mean', 'std']  # You can directly input statistics names\n",
    "stats = iterators.Stats.get_keys()  # If you need to calculate all supported statistics\n",
    "splitter = splitter.Splitter(use_signal=True, use_specter=True, specter_threshold=1000, stats=stats)\n",
    "prepared_data = splitter.split_dataset(joined_dataset, stable_area=(10, 19), splits_number=10,\n",
    "                                       signal_data_columns=['a1_x', 'a1_y', 'a1_z', 'a2_x', 'a2_y', 'a2_z'])\n",
    "print(f\"features number: {prepared_data.shape[1]-2}\")\n",
    "print(f\"examples number: {prepared_data.shape[0]}\")\n",
    "print(prepared_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Run ML experiment\n",
    "As an example, cross-validation with grouped overlap resampling launched here over linear regression, SVC and random forest classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialize experiment workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from source.processes import Shuffler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = prepared_data.drop(columns=['target', 'group'])\n",
    "y = prepared_data['target']\n",
    "groups = prepared_data['group']\n",
    "\n",
    "cv = Shuffler.OverlapGroupCV(train_size=0.7, n_repeats=100)\n",
    "logit = LogisticRegression(C=0.01)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "scores = iterators.Metrics.get_scorers_dict()  # Get dict of scores in format required by cross_validate() scoring field\n",
    "\n",
    "print(X.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(logit, X_scaled, y, cv=cv, scoring=scores)\n",
    "print(sorted(cv_results.keys()))\n",
    "\n",
    "print(cv_results['test_score'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09a5e7c8b183e7b2d0a3e79941b92934b636b4d366df9a66eaddd71cb8199dab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
