{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from source.preprocessing import splitter, converter\n",
    "from source.datamodels import iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1 Preprocessing\n",
    "load datasets, convert third-party data files to our format etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "project_folder = \"F:/PythonNotebooks/Study/Quantum/Bearings/\"\n",
    "own_data_path = os.path.join(project_folder, \"data/own datasets/\")\n",
    "third_party_data_path = os.path.join(project_folder, \"data/third party datasets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1.1 Load our initial datasets\n",
    "Datasets obtained from our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signals_dataset = pd.read_csv(os.path.join(own_data_path, 'bearing_signals.csv'))\n",
    "classes_dataset = pd.read_csv(os.path.join(own_data_path, 'bearing_classes.csv'), delimiter=';', skiprows=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.2 Load our joined datasets\n",
    "Datasets obtained from our experiments with joined target column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "full_dataset = pd.read_csv(os.path.join(own_data_path, 'bearings.csv'), delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1.3 Load third-party datasets\n",
    "Load third-party datasets, converted to our standard view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_dataset = pd.read_csv(os.path.join(third_party_data_path, 'N1 Cesar Ricardo', 'csv', 'bearings.csv'), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.4 Load dataset with statistics\n",
    "Dataset is ready for experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   target  group  a1_x_signal_complexity  a1_x_signal_shannon_entropy  \\\n0     0.0    1.0                1.865568                     6.388020   \n1     0.0    1.0                1.812863                     6.507361   \n2     0.0    1.0                1.775775                     6.489806   \n3     0.0    1.0                1.912905                     6.422261   \n4     0.0    1.0                1.868826                     6.449559   \n\n   a1_x_signal_kurtosis  a1_x_signal_variation  a1_x_signal_hurst  \\\n0             -0.337526              -9.989303           0.626170   \n1             -0.329388              -8.479932           0.633813   \n2             -0.439591              -9.585973           0.625745   \n3             -0.364524              -9.033494           0.606942   \n4              0.017384              -8.482620           0.631058   \n\n   a1_x_signal_skew  a1_x_signal_activity  a1_x_signal_iqr  ...  \\\n0         -0.231906              0.519530         3.006134  ...   \n1          0.046480              0.519920         3.270674  ...   \n2          0.014204              0.531458         3.174478  ...   \n3         -0.145760              0.512090         2.982085  ...   \n4         -0.149621              0.526677         2.982085  ...   \n\n   a2_z_specter_iqr  a2_z_specter_zero_crossing  a2_z_specter_range  \\\n0        130.212247                         0.0          430.516204   \n1         79.932721                         0.0          339.669032   \n2         87.487539                         0.0          359.727237   \n3         78.688634                         0.0          353.579137   \n4         83.595954                         0.0          331.560162   \n\n   a2_z_specter_mean  a2_z_specter_petrosian_fd  a2_z_specter_higuchi_fd  \\\n0         121.609951                   1.024136                 1.709899   \n1         116.951076                   1.024806                 1.754620   \n2         126.969372                   1.022686                 1.718792   \n3         108.447712                   1.025371                 1.746161   \n4         112.269206                   1.024240                 1.762398   \n\n   a2_z_specter_crest_factor  a2_z_specter_energy  a2_z_specter_std  \\\n0                   2.865578         2.261059e+07         88.439848   \n1                   2.630547         1.683235e+07         56.167537   \n2                   2.593399         1.971760e+07         59.969811   \n3                   2.880197         1.547513e+07         60.944452   \n4                   2.665190         1.596652e+07         57.984006   \n\n   a2_z_specter_sample_entropy  \n0                     2.754299  \n1                     2.881832  \n2                     2.874073  \n3                     2.780298  \n4                     2.852191  \n\n[5 rows x 206 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>group</th>\n      <th>a1_x_signal_complexity</th>\n      <th>a1_x_signal_shannon_entropy</th>\n      <th>a1_x_signal_kurtosis</th>\n      <th>a1_x_signal_variation</th>\n      <th>a1_x_signal_hurst</th>\n      <th>a1_x_signal_skew</th>\n      <th>a1_x_signal_activity</th>\n      <th>a1_x_signal_iqr</th>\n      <th>...</th>\n      <th>a2_z_specter_iqr</th>\n      <th>a2_z_specter_zero_crossing</th>\n      <th>a2_z_specter_range</th>\n      <th>a2_z_specter_mean</th>\n      <th>a2_z_specter_petrosian_fd</th>\n      <th>a2_z_specter_higuchi_fd</th>\n      <th>a2_z_specter_crest_factor</th>\n      <th>a2_z_specter_energy</th>\n      <th>a2_z_specter_std</th>\n      <th>a2_z_specter_sample_entropy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.865568</td>\n      <td>6.388020</td>\n      <td>-0.337526</td>\n      <td>-9.989303</td>\n      <td>0.626170</td>\n      <td>-0.231906</td>\n      <td>0.519530</td>\n      <td>3.006134</td>\n      <td>...</td>\n      <td>130.212247</td>\n      <td>0.0</td>\n      <td>430.516204</td>\n      <td>121.609951</td>\n      <td>1.024136</td>\n      <td>1.709899</td>\n      <td>2.865578</td>\n      <td>2.261059e+07</td>\n      <td>88.439848</td>\n      <td>2.754299</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.812863</td>\n      <td>6.507361</td>\n      <td>-0.329388</td>\n      <td>-8.479932</td>\n      <td>0.633813</td>\n      <td>0.046480</td>\n      <td>0.519920</td>\n      <td>3.270674</td>\n      <td>...</td>\n      <td>79.932721</td>\n      <td>0.0</td>\n      <td>339.669032</td>\n      <td>116.951076</td>\n      <td>1.024806</td>\n      <td>1.754620</td>\n      <td>2.630547</td>\n      <td>1.683235e+07</td>\n      <td>56.167537</td>\n      <td>2.881832</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.775775</td>\n      <td>6.489806</td>\n      <td>-0.439591</td>\n      <td>-9.585973</td>\n      <td>0.625745</td>\n      <td>0.014204</td>\n      <td>0.531458</td>\n      <td>3.174478</td>\n      <td>...</td>\n      <td>87.487539</td>\n      <td>0.0</td>\n      <td>359.727237</td>\n      <td>126.969372</td>\n      <td>1.022686</td>\n      <td>1.718792</td>\n      <td>2.593399</td>\n      <td>1.971760e+07</td>\n      <td>59.969811</td>\n      <td>2.874073</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.912905</td>\n      <td>6.422261</td>\n      <td>-0.364524</td>\n      <td>-9.033494</td>\n      <td>0.606942</td>\n      <td>-0.145760</td>\n      <td>0.512090</td>\n      <td>2.982085</td>\n      <td>...</td>\n      <td>78.688634</td>\n      <td>0.0</td>\n      <td>353.579137</td>\n      <td>108.447712</td>\n      <td>1.025371</td>\n      <td>1.746161</td>\n      <td>2.880197</td>\n      <td>1.547513e+07</td>\n      <td>60.944452</td>\n      <td>2.780298</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.868826</td>\n      <td>6.449559</td>\n      <td>0.017384</td>\n      <td>-8.482620</td>\n      <td>0.631058</td>\n      <td>-0.149621</td>\n      <td>0.526677</td>\n      <td>2.982085</td>\n      <td>...</td>\n      <td>83.595954</td>\n      <td>0.0</td>\n      <td>331.560162</td>\n      <td>112.269206</td>\n      <td>1.024240</td>\n      <td>1.762398</td>\n      <td>2.665190</td>\n      <td>1.596652e+07</td>\n      <td>57.984006</td>\n      <td>2.852191</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 206 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_data = pd.read_csv(os.path.join(own_data_path, 'processed_full_signal_specter1000_noscale.csv'), delimiter=',')  # our experiment\n",
    "# prepared_data = pd.read_csv(os.path.join(third_party_data_path, 'N1 Cesar Ricardo', 'csv',\n",
    "#                                          'processed_full_signal_specter1000_noscale.csv'), delimiter=',')  # third-party dataset\n",
    "prepared_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "## 1.2 Signals and classes datasets join\n",
    "Use to combine our datasets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "targets_map = dict(zip(classes_dataset['bearing_id'], classes_dataset['status']))\n",
    "targets_vector = signals_dataset['bearing_2_id'].map(targets_map)\n",
    "joined_dataset = signals_dataset.copy()\n",
    "joined_dataset.insert(loc=0, column='target', value=targets_vector)\n",
    "joined_dataset.to_csv(os.path.join(own_data_path, 'bearings.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "## 1.3 Convert third-party data files to our standard dataframe view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 44/45 [00:42<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "   target      a1_y      a2_y  rpm  experiment_id  timestamp\n0       1  1.937934  1.954861  200              1   0.000000\n1       1  1.937547  1.954629  200              1   0.000025\n2       1  1.937166  1.954989  200              1   0.000050\n3       1  1.937594  1.955540  200              1   0.000075\n4       1  1.938502  1.955792  200              1   0.000100",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>a1_y</th>\n      <th>a2_y</th>\n      <th>rpm</th>\n      <th>experiment_id</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.937934</td>\n      <td>1.954861</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1.937547</td>\n      <td>1.954629</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.000025</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1.937166</td>\n      <td>1.954989</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.000050</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1.937594</td>\n      <td>1.955540</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.000075</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1.938502</td>\n      <td>1.955792</td>\n      <td>200</td>\n      <td>1</td>\n      <td>0.000100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cesar_1_path = os.path.join(third_party_data_path, 'N1 Cesar Ricardo/')\n",
    "cesar_1 = converter.Converter.cesar_convert(cesar_1_path)\n",
    "cesar_1.head()\n",
    "\n",
    "# cesar_2_path = os.path.join(third_party_data_path, 'Bearings_cesar_1')\n",
    "# cesar_2 = converter.Converter.cesar_convert(cesar_2_path)\n",
    "\n",
    "# luigi_path = os.path.join(third_party_data_path, 'Bearings_luigi')\n",
    "# luigi = converter.Converter.luigi_convert(luigi_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "## 1.4 Split datasets\n",
    "Split datasets on chunks and evaluate set of statistical features for each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.1 Split our dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# stats = ['mean', 'std']  # You can directly input statistics names\n",
    "stats = iterators.Stats.get_keys()  # Use Stats.get_keys() if you need to calculate all supported statistics\n",
    "splitter_processor = splitter.Splitter(use_signal=True, use_specter=True, specter_threshold=1000, stats=stats)\n",
    "prepared_data = splitter_processor.split_dataset(joined_dataset, stable_area=(0, 3), splits_number=10,\n",
    "                                                 signal_data_columns=['a1_x', 'a1_y', 'a1_z', 'a2_x', 'a2_y', 'a2_z'])\n",
    "print(f\"features number: {prepared_data.shape[1]-2}\")\n",
    "print(f\"examples number: {prepared_data.shape[0]}\")\n",
    "print(prepared_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.2 Split third-party dataset\n",
    "For datasets created by César Ricardo Soto-Ocampo et al. there is a1_y and a2_y columns. For this example we use stable timezone from 0 to 3 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features number: 68\n",
      "examples number: 440\n",
      "   target  group  a1_y_signal_activity  a1_y_signal_energy  a1_y_signal_iqr  \\\n",
      "0     1.0    1.0              0.041605        44338.707155         0.026614   \n",
      "1     1.0    1.0              0.133815        44540.227042         0.001980   \n",
      "2     1.0    1.0              0.470884        44653.643007         0.001556   \n",
      "3     1.0    1.0              0.870449        44680.818980         0.001434   \n",
      "4     1.0    1.0              0.460420        44708.987425         0.001382   \n",
      "\n",
      "   a1_y_signal_variation  a1_y_signal_mean  a1_y_signal_range  \\\n",
      "0               0.006545          1.922169           0.042966   \n",
      "1               0.002293          1.926568           0.037050   \n",
      "2               0.000625          1.929024           0.020445   \n",
      "3               0.000777          1.929611           0.061155   \n",
      "4               0.000564          1.930220           0.012463   \n",
      "\n",
      "   a1_y_signal_std  a1_y_signal_complexity  ...  a2_y_specter_complexity  \\\n",
      "0         0.012580               31.661320  ...                 3.417990   \n",
      "1         0.004417               10.516747  ...                 5.375270   \n",
      "2         0.001207                2.934539  ...                 4.808027   \n",
      "3         0.001498                1.831956  ...                 4.470592   \n",
      "4         0.001088                2.952793  ...                 3.973786   \n",
      "\n",
      "   a2_y_specter_skew  a2_y_specter_sample_entropy  a2_y_specter_kurtosis  \\\n",
      "0          -0.112285                     2.868557              -0.034169   \n",
      "1          -0.797098                     2.805931               0.636421   \n",
      "2           0.067012                     2.863283              -0.084551   \n",
      "3          -0.215543                     2.867958              -0.321073   \n",
      "4           0.454384                     2.867852               0.251929   \n",
      "\n",
      "   a2_y_specter_hurst  a2_y_specter_petrosian_fd  a2_y_specter_zero_crossing  \\\n",
      "0            0.788408                   1.025012                         0.0   \n",
      "1            0.657959                   1.025371                         0.0   \n",
      "2            0.677378                   1.024806                         0.0   \n",
      "3            0.713769                   1.026344                         0.0   \n",
      "4            0.749379                   1.024600                         0.0   \n",
      "\n",
      "   a2_y_specter_crest_factor  a2_y_specter_higuchi_fd  \\\n",
      "0                   1.000277                 1.608481   \n",
      "1                   1.305359                 1.601905   \n",
      "2                   1.175973                 1.593797   \n",
      "3                   1.266530                 1.613716   \n",
      "4                   1.364315                 1.535780   \n",
      "\n",
      "   a2_y_specter_shannon_entropy  \n",
      "0                      9.965784  \n",
      "1                      9.965784  \n",
      "2                      9.965784  \n",
      "3                      9.965784  \n",
      "4                      9.965784  \n",
      "\n",
      "[5 rows x 70 columns]\n",
      "Wall time: 1h 45min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined_dataset = cesar_1\n",
    "stats = iterators.Stats.get_keys()\n",
    "splitter_processor = splitter.Splitter(use_signal=True, use_specter=True, specter_threshold=1000, stats=stats)\n",
    "prepared_data = splitter_processor.split_dataset(joined_dataset, stable_area=[(0, 3)], splits_number=10,\n",
    "                                                 signal_data_columns=['a1_y', 'a2_y'])\n",
    "print(f\"features number: {prepared_data.shape[1]-2}\")\n",
    "print(f\"examples number: {prepared_data.shape[0]}\")\n",
    "prepared_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2 Run ML experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1 Run cross-validation\n",
    "As an example, cross-validation with grouped overlap resampling launched here over logistic regression, SVC and random forest classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2.1 Initialize experiment workflow and estimators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from source.processes import Shuffler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(n_estimators=200)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_params = {'C': 10}\n",
    "logit = LogisticRegression()\n",
    "logit.set_params(**LR_params)\n",
    "\n",
    "SVC_params = {'C': 10}\n",
    "svc = SVC()\n",
    "svc.set_params(**SVC_params)\n",
    "\n",
    "RFC_params = {'n_estimators': 200}\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.set_params(**RFC_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(1120, 204)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = prepared_data.drop(columns=['target', 'group']).values\n",
    "y = prepared_data['target'].values\n",
    "groups = prepared_data['group'].values\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "scores = iterators.Metrics.get_scorers_dict()  # Get dict of scores in format required by cross_validate() scoring field\n",
    "\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 run cross-validations for each estimator\n",
    "We run 100 fits for each estimator in this example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression mean F1 score: 0.841052024168488\n",
      "Wall time: 6.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = Shuffler.OverlapGroupCV(train_size=0.63, n_repeats=100).split(X_scaled, y, groups)\n",
    "logit_cv_results = cross_validate(logit, X_scaled, y, cv=cv, scoring=scores, groups=groups)\n",
    "print(f\"Logistic regression mean F1 score: {np.mean(logit_cv_results['test_f1'])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC mean F1 score: 0.7445571254312032\n",
      "Wall time: 5.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = Shuffler.OverlapGroupCV(train_size=0.63, n_repeats=100).split(X_scaled, y, groups)\n",
    "svc_cv_results = cross_validate(svc, X_scaled, y, cv=cv, scoring=scores, groups=groups)\n",
    "print(f\"SVC mean F1 score: {np.mean(svc_cv_results['test_f1'])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest mean F1 score: 0.7327410602124919\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = Shuffler.OverlapGroupCV(train_size=0.63, n_repeats=100).split(X_scaled, y, groups)\n",
    "rfc_cv_results = cross_validate(rfc, X_scaled, y, cv=cv, scoring=scores, groups=groups)\n",
    "print(f\"Random forest mean F1 score: {np.mean(rfc_cv_results['test_f1'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Run GridSearch\n",
    "GridSearch for Logistic Regression tuning with bootstrapped samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from source.processes import Shuffler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "X = prepared_data.drop(columns=['target', 'group']).values\n",
    "y = prepared_data['target'].values\n",
    "groups = prepared_data['group'].values\n",
    "\n",
    "logit = LogisticRegression()\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "cv = Shuffler.OverlapGroupCV(train_size=0.63, n_repeats=100).split(X_scaled, y, groups)\n",
    "grid = {'C': np.logspace(-3, 4, 8)}\n",
    "gscv = GridSearchCV(logit, grid, scoring='f1', cv=cv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass groups=[  1.   1.   1. ... 112. 112. 112.] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10000.0}\n",
      "0.8749091756161085\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gscv.fit(X_scaled, y, groups)\n",
    "print(gscv.best_params_)\n",
    "print(gscv.best_score_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Results postprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 extract bootstrap scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.1 extract CV scores\n",
    "There we extract scores, obtained in stage 2.2.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR mean scores: {'accuracy': 0.9672857142857141, 'precision': 0.9600499700732599, 'recall': 0.7603999999999997, 'f1': 0.841052024168488, 'TPR': 0.7603999999999997, 'TNR': 0.9952432432432431}\n",
      "SVC mean scores: {'accuracy': 0.9509047619047619, 'precision': 0.9424801908679588, 'recall': 0.6327999999999999, 'f1': 0.7445571254312032, 'TPR': 0.6327999999999999, 'TNR': 0.9938918918918919}\n",
      "RFC mean scores: {'accuracy': 0.9518095238095239, 'precision': 0.9979512288786482, 'recall': 0.596, 'f1': 0.7327410602124919, 'TPR': 0.596, 'TNR': 0.9998918918918919}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cv_scores_names = [f'test_{score}' for score in scores.keys()]\n",
    "\n",
    "logit_cv_scores ={re.sub(\"test_\",\"\", k):list(logit_cv_results[k]) for k in cv_scores_names}\n",
    "logit_mean_scores = { k:np.mean(logit_cv_scores[k]) for k in logit_cv_scores.keys()}\n",
    "print(f\"LR mean scores: {logit_mean_scores}\")\n",
    "\n",
    "svc_cv_scores ={re.sub(\"test_\",\"\", k):list(svc_cv_results[k]) for k in cv_scores_names}\n",
    "svc_mean_scores = { k:np.mean(svc_cv_scores[k]) for k in svc_cv_scores.keys()}\n",
    "print(f\"SVC mean scores: {svc_mean_scores}\")\n",
    "\n",
    "rfc_cv_scores ={re.sub(\"test_\",\"\", k):list(rfc_cv_results[k]) for k in cv_scores_names}\n",
    "rfc_mean_scores = { k:np.mean(rfc_cv_scores[k]) for k in rfc_cv_scores.keys()}\n",
    "print(f\"RFC mean scores: {rfc_mean_scores}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Save experiment data to data model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 Save cross-validation results\n",
    "We save cv results as BootstrapResults instances to further serialization (stage 3.2.3) and ML experiment tracking tables creation (stage 3.2.4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from source.datamodels.datamodels import BootstrapResults\n",
    "from source.datamodels.iterators import Axes, Stats\n",
    "\n",
    "logit_result_obj = BootstrapResults(\n",
    "    run_label = \"test cv run\",\n",
    "    model_name = \"LR\",\n",
    "    hyperparameters = LR_params,\n",
    "    use_signal = True,\n",
    "    use_specter = True,\n",
    "    specter_threshold = 1000,\n",
    "    axes = Axes.get_keys(),\n",
    "    stats = Stats.get_keys(),\n",
    "    predictions = None,\n",
    "    scores = logit_mean_scores,\n",
    "    resampling_number = 100,\n",
    "    bootstrap_scores = logit_cv_scores\n",
    ")\n",
    "\n",
    "svc_result_obj = BootstrapResults(\n",
    "    run_label = \"test cv run\",\n",
    "    model_name = \"SVC\",\n",
    "    hyperparameters = SVC_params,\n",
    "    use_signal = True,\n",
    "    use_specter = True,\n",
    "    specter_threshold = 1000,\n",
    "    axes = Axes.get_keys(),\n",
    "    stats = Stats.get_keys(),\n",
    "    predictions = None,\n",
    "    scores = svc_mean_scores,\n",
    "    resampling_number = 100,\n",
    "    bootstrap_scores = svc_cv_scores\n",
    ")\n",
    "\n",
    "rfc_result_obj = BootstrapResults(\n",
    "    run_label = \"test cv run\",\n",
    "    model_name = \"RFC\",\n",
    "    hyperparameters = RFC_params,\n",
    "    use_signal = True,\n",
    "    use_specter = True,\n",
    "    specter_threshold = 1000,\n",
    "    axes = Axes.get_keys(),\n",
    "    stats = Stats.get_keys(),\n",
    "    predictions = None,\n",
    "    scores = rfc_mean_scores,\n",
    "    resampling_number = 100,\n",
    "    bootstrap_scores = rfc_cv_scores\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 Plot results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from source.postprocessing.plotter import bar_plot\n",
    "\n",
    "bar_plot(results=[logit_mean_scores, rfc_mean_scores, svc_mean_scores], models=['LR', 'RF', 'SVM'], metrics=list(logit_mean_scores.keys()), plot_size=(8, 12), Title='Test CV mean scores', filename='test_cv_mean_scores.png', filepath=os.path.join(project_folder, 'experiments/images'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.3 Results serialization\n",
    "We serialize objects obtained in stage 3.2.1 for full reproducibility of experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from source.postprocessing.mljson import serialize_results\n",
    "\n",
    "\n",
    "CV_results_path = os.path.join(project_folder, \"experiments/BootstrapsRuns/CV/\")\n",
    "ResultTablesPath = os.path.join(project_folder, \"experiments/ResultTables/\")\n",
    "results_objects=[logit_result_obj, svc_result_obj, rfc_result_obj]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "serialize_results(results=results_objects, filenames=['logit_test.json', 'svc_test.json', 'rfc_test.json'], filepath=CV_results_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.4 Write results to Excel table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from source.postprocessing.mlcsv import generate_csv_from_results, create_readable_xlsx\n",
    "\n",
    "\n",
    "generate_csv_from_results(results_objects, csv_name='csv_test.csv', results_type=BootstrapResults, csv_path=ResultTablesPath)\n",
    "create_readable_xlsx(xlsx_name='xlsx_test.xlsx', csv_name='csv_test.csv', xlsx_path=ResultTablesPath, csv_path=ResultTablesPath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "   experiment index    run_label model_name use_signal use_specter  \\\n0                 0  test cv run         LR        Yes         Yes   \n1                 1  test cv run        SVC        Yes         Yes   \n2                 2  test cv run        RFC        Yes         Yes   \n\n   specter_threshold  resampling_number Scores: accuracy Scores: precision  \\\n0               1000                100             0,97             0,958   \n1               1000                100             0,95             0,949   \n2               1000                100            0,955               1,0   \n\n  Scores: recall  ... Statistics: energy Statistics: hurst  \\\n0          0,788  ...                Yes               Yes   \n1          0,622  ...                Yes               Yes   \n2          0,624  ...                Yes               Yes   \n\n  Statistics: petrosian_fd Statistics: zero_crossing Statistics: higuchi_fd  \\\n0                      Yes                       Yes                    Yes   \n1                      Yes                       Yes                    Yes   \n2                      Yes                       Yes                    Yes   \n\n  Statistics: activity Statistics: complexity Statistics: crest_factor  \\\n0                  Yes                    Yes                      Yes   \n1                  Yes                    Yes                      Yes   \n2                  Yes                    Yes                      Yes   \n\n  Hyperparameters: C Hyperparameters: n_estimators  \n0               10,0                           NaN  \n1               10,0                           NaN  \n2                NaN                         200,0  \n\n[3 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>experiment index</th>\n      <th>run_label</th>\n      <th>model_name</th>\n      <th>use_signal</th>\n      <th>use_specter</th>\n      <th>specter_threshold</th>\n      <th>resampling_number</th>\n      <th>Scores: accuracy</th>\n      <th>Scores: precision</th>\n      <th>Scores: recall</th>\n      <th>...</th>\n      <th>Statistics: energy</th>\n      <th>Statistics: hurst</th>\n      <th>Statistics: petrosian_fd</th>\n      <th>Statistics: zero_crossing</th>\n      <th>Statistics: higuchi_fd</th>\n      <th>Statistics: activity</th>\n      <th>Statistics: complexity</th>\n      <th>Statistics: crest_factor</th>\n      <th>Hyperparameters: C</th>\n      <th>Hyperparameters: n_estimators</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>test cv run</td>\n      <td>LR</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>0,97</td>\n      <td>0,958</td>\n      <td>0,788</td>\n      <td>...</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>10,0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>test cv run</td>\n      <td>SVC</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>0,95</td>\n      <td>0,949</td>\n      <td>0,622</td>\n      <td>...</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>10,0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>test cv run</td>\n      <td>RFC</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>0,955</td>\n      <td>1,0</td>\n      <td>0,624</td>\n      <td>...</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>NaN</td>\n      <td>200,0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 38 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_excel(os.path.join(ResultTablesPath, 'xlsx_test.xlsx'))\n",
    "table.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09a5e7c8b183e7b2d0a3e79941b92934b636b4d366df9a66eaddd71cb8199dab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}